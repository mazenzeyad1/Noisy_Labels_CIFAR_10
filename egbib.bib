
@misc{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {✅},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/jeremy/Zotero/storage/M2XGLWFC/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{tanEfficientNetRethinkingModel2020,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://arxiv.org/abs/1905.11946},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3\% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Tan, Mingxing and Le, Quoc V.},
	month = sep,
	year = {2020},
	note = {arXiv:1905.11946 [cs, stat]},
	keywords = {✅},
	file = {Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:/home/jeremy/Zotero/storage/55IQUWSB/Tan and Le - 2020 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf},
}

@misc{huSqueezeandExcitationNetworks2019,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of {\textasciitilde}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = may,
	year = {2019},
	note = {arXiv:1709.01507 [cs]},
	keywords = {✅, CNN, SENet},
	file = {Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:/home/jeremy/Zotero/storage/Q7JQVP8T/Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:application/pdf},
}

@misc{xieAggregatedResidualTransformations2017,
	title = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.05431},
	abstract = {We present a simple, highly modularized network architecture for image classiﬁcation. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call “cardinality” (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classiﬁcation accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classiﬁcation task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online1.},
	language = {en},
	urldate = {2023-06-10},
	publisher = {arXiv},
	author = {Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},
	month = apr,
	year = {2017},
	note = {arXiv:1611.05431 [cs]},
	keywords = {✅, CNN, Inception, Resnet, ResNext},
	file = {Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:/home/jeremy/Zotero/storage/JU5KI3VT/Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf:application/pdf},
}

@misc{cholletXceptionDeepLearning2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2023-05-25},
	publisher = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]},
	keywords = {✅, CNN, Inception, Depthwise Separable Convolution},
	file = {Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:/home/jeremy/Zotero/storage/LGVJTVG8/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf},
}

@misc{simonyanVeryDeepConvolutional2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition} ({VGG})},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {✅, CNN, VGG},
	file = {arXiv.org Snapshot:/home/jeremy/Zotero/storage/H4QF66ZY/1409.html:text/html;Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/home/jeremy/Zotero/storage/TPEC2W7N/Simonyan_Zisserman_2015_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf},
}

@misc{huangDenselyConnectedConvolutional2018,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	doi = {10.48550/arXiv.1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jan,
	year = {2018},
	note = {arXiv:1608.06993 [cs]},
	keywords = {✅, CNN, DenseNet},
	file = {arXiv.org Snapshot:/home/jeremy/Zotero/storage/Q76JJ87Z/1608.html:text/html;Huang et al_2018_Densely Connected Convolutional Networks.pdf:/home/jeremy/Zotero/storage/WGG4GQMF/Huang et al_2018_Densely Connected Convolutional Networks.pdf:application/pdf},
}

@article{szegedyInceptionv4InceptionResNetImpact2017,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	volume = {31},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11231},
	doi = {10.1609/aaai.v31i1.11231},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
	language = {en},
	number = {1},
	urldate = {2023-05-29},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
	month = feb,
	year = {2017},
	note = {Number: 1},
	keywords = {🔴},
	file = {Szegedy et al_2017_Inception-v4, Inception-ResNet and the Impact of Residual Connections on.pdf:/home/jeremy/Zotero/storage/QVVHZS9I/Szegedy et al_2017_Inception-v4, Inception-ResNet and the Impact of Residual Connections on.pdf:application/pdf},
}

@article{krizhevskyImageNetClassificationDeep2017,
	title = {{ImageNet} classification with deep convolutional neural networks},
	volume = {60},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3065386},
	doi = {10.1145/3065386},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	language = {en},
	number = {6},
	urldate = {2023-05-28},
	journal = {Communications of the ACM},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	month = may,
	year = {2017},
	keywords = {✅, CNN, AlexNet},
	pages = {84--90},
	file = {Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:/home/jeremy/Zotero/storage/P7D3QMGJ/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf:application/pdf},
}

@misc{sandlerMobileNetV2InvertedResiduals2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
	urldate = {2023-05-27},
	publisher = {arXiv},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv:1801.04381 [cs]},
	keywords = {🔴},
	file = {Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:/home/jeremy/Zotero/storage/5MKVSRRH/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@misc{chenSamplePriorGuided2022,
	title = {Sample {Prior} {Guided} {Robust} {Model} {Learning} to {Suppress} {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2112.01197},
	abstract = {Imperfect labels are ubiquitous in real-world datasets and seriously harm the model performance. Several recent effective methods for handling noisy labels have two key steps: 1) dividing samples into cleanly labeled and wrongly labeled sets by training loss, 2) using semi-supervised methods to generate pseudo-labels for samples in the wrongly labeled set. However, current methods always hurt the informative hard samples due to the similar loss distribution between the hard samples and the noisy ones. In this paper, we proposed PGDF (Prior Guided Denoising Framework), a novel framework to learn a deep model to suppress noise by generating the samples’ prior knowledge, which is integrated into both dividing samples step and semi-supervised step. Our framework can save more informative hard clean samples into the cleanly labeled set. Besides, our framework also promotes the quality of pseudo-labels during the semi-supervised step by suppressing the noise in the current pseudo-labels generating scheme. To further enhance the hard samples, we reweight the samples in the cleanly labeled set during training. We evaluated our method using synthetic datasets based on CIFAR-10 and CIFAR-100, as well as on the real-world datasets WebVision and Clothing1M. The results demonstrate substantial improvements over state-of-the-art methods.},
	language = {en},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Chen, Wenkai and Zhu, Chuang and Chen, Yi and Li, Mengting and Huang, Tiejun},
	month = jan,
	year = {2022},
	note = {arXiv:2112.01197 [cs]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.4.0, I.5.1},
	file = {Chen et al. - 2022 - Sample Prior Guided Robust Model Learning to Suppr.pdf:/home/jeremy/Zotero/storage/Z6XY8ZUL/Chen et al. - 2022 - Sample Prior Guided Robust Model Learning to Suppr.pdf:application/pdf},
}

@misc{songLearningNoisyLabels2022,
	title = {Learning from {Noisy} {Labels} with {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Learning from {Noisy} {Labels} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2007.08199},
	abstract = {Deep learning has achieved remarkable success in numerous domains with help from large amounts of big data. However, the quality of data labels is a concern because of the lack of high-quality labels in many real-world scenarios. As noisy labels severely degrade the generalization performance of deep neural networks, learning from noisy labels (robust training) is becoming an important task in modern deep learning applications. In this survey, we ﬁrst describe the problem of learning with label noise from a supervised learning perspective. Next, we provide a comprehensive review of 62 state-of-the-art robust training methods, all of which are categorized into ﬁve groups according to their methodological difference, followed by a systematic comparison of six properties used to evaluate their superiority. Subsequently, we perform an in-depth analysis of noise rate estimation and summarize the typically used evaluation methodology, including public noisy datasets and evaluation metrics. Finally, we present several promising research directions that can serve as a guideline for future studies. All the contents will be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.},
	language = {en},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
	month = mar,
	year = {2022},
	note = {arXiv:2007.08199 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Song et al. - 2022 - Learning from Noisy Labels with Deep Neural Networ.pdf:/home/jeremy/Zotero/storage/CIEQP7UR/Song et al. - 2022 - Learning from Noisy Labels with Deep Neural Networ.pdf:application/pdf},
}

@misc{zhangShuffleNetExtremelyEfficient2017,
	title = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
	shorttitle = {{ShuffleNet}},
	url = {http://arxiv.org/abs/1707.01083},
	doi = {10.48550/arXiv.1707.01083},
	abstract = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves {\textasciitilde}13x actual speedup over AlexNet while maintaining comparable accuracy.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
	month = dec,
	year = {2017},
	note = {arXiv:1707.01083 [cs]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/KES8JP6E/Zhang et al. - 2017 - ShuffleNet An Extremely Efficient Convolutional N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/YUM7YTYF/1707.html:text/html},
}

@article{lecunGradientbasedLearningApplied1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	url = {https://ieeexplore.ieee.org/document/726791},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	urldate = {2023-11-25},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {🔴},
	pages = {2278--2324},
	file = {IEEE Xplore Abstract Record:/home/jeremy/Zotero/storage/YN4AABRZ/726791.html:text/html},
}

@inproceedings{tongxiaoLearningMassiveNoisy2015,
	address = {Boston, MA, USA},
	title = {Learning from massive noisy labeled data for image classification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298885/},
	doi = {10.1109/CVPR.2015.7298885},
	abstract = {Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classiﬁcation dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.},
	language = {en},
	urldate = {2023-11-25},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {{Tong Xiao} and {Tian Xia} and {Yi Yang} and {Chang Huang} and {Xiaogang Wang}},
	month = jun,
	year = {2015},
	keywords = {🔴},
	pages = {2691--2699},
	file = {Tong Xiao et al. - 2015 - Learning from massive noisy labeled data for image.pdf:/home/jeremy/Zotero/storage/XKJJGK7B/Tong Xiao et al. - 2015 - Learning from massive noisy labeled data for image.pdf:application/pdf},
}

@inproceedings{weiOpensetLabelNoise2021,
	title = {Open-set {Label} {Noise} {Can} {Improve} {Robustness} {Against} {Inherent} {Label} {Noise}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/428fca9bc1921c25c5121f9da7815cde-Abstract.html},
	abstract = {Learning with noisy labels is a practically challenging problem in weakly supervised learning. In the existing literature, open-set noises are always considered to be poisonous for generalization, similar to closed-set noises. In this paper, we empirically show that open-set noisy labels can be non-toxic and even benefit the robustness against inherent noisy labels. Inspired by the observations, we propose a simple yet effective regularization by introducing Open-set samples with Dynamic Noisy Labels (ODNL) into training. With ODNL, the extra capacity of the neural network can be largely consumed in a way that does not interfere with learning patterns from clean data. Through the lens of SGD noise, we show that the noises induced by our method are random-direction, conflict-free and biased, which may help the model converge to a flat minimum with superior stability and enforce the model to produce conservative predictions on Out-of-Distribution instances. Extensive experimental results on benchmark datasets with various types of noisy labels demonstrate that the proposed method not only enhances the performance of many existing robust algorithms but also achieves significant improvement on Out-of-Distribution detection tasks even in the label noise setting.},
	urldate = {2023-11-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wei, Hongxin and Tao, Lue and XIE, RENCHUNZI and An, Bo},
	year = {2021},
	keywords = {🔴},
	pages = {7978--7992},
	file = {Full Text PDF:/home/jeremy/Zotero/storage/TPLN8ZHX/Wei et al. - 2021 - Open-set Label Noise Can Improve Robustness Agains.pdf:application/pdf},
}

@misc{zhangMixupEmpiricalRisk2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	shorttitle = {mixup},
	url = {http://arxiv.org/abs/1710.09412},
	doi = {10.48550/arXiv.1710.09412},
	abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	month = apr,
	year = {2018},
	note = {arXiv:1710.09412 [cs, stat]},
	keywords = {🔴, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/TD8Z8G7Y/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/GI2KZ2E4/1710.html:text/html},
}

@misc{maNormalizedLossFunctions2020,
	title = {Normalized {Loss} {Functions} for {Deep} {Learning} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2006.13554},
	doi = {10.48550/arXiv.2006.13554},
	abstract = {Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: any loss can be made robust to noisy labels. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of underfitting. To address this, we propose a framework to build robust loss functions called Active Passive Loss (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60\% or 80\% incorrect labels.},
	urldate = {2023-11-25},
	publisher = {arXiv},
	author = {Ma, Xingjun and Huang, Hanxun and Wang, Yisen and Romano, Simone and Erfani, Sarah and Bailey, James},
	month = jun,
	year = {2020},
	note = {arXiv:2006.13554 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/JBT2P6X5/Ma et al. - 2020 - Normalized Loss Functions for Deep Learning with N.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/JS7A7PAR/2006.html:text/html},
}

@inproceedings{songRobustLearningSelfTransition2021,
	title = {Robust {Learning} by {Self}-{Transition} for {Handling} {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2012.04337},
	doi = {10.1145/3447548.3467222},
	abstract = {Real-world data inevitably contains noisy labels, which induce the poor generalization of deep neural networks. It is known that the network typically begins to rapidly memorize false-labeled samples after a certain point of training. Thus, to counter the label noise challenge, we propose a novel self-transitional learning method called MORPH, which automatically switches its learning phase at the transition point from seeding to evolution. In the seeding phase, the network is updated using all the samples to collect a seed of clean samples. Then, in the evolution phase, the network is updated using only the set of arguably clean samples, which precisely keeps expanding by the updated network. Thus, MORPH effectively avoids the overfitting to false-labeled samples throughout the entire training period. Extensive experiments using five real-world or synthetic benchmark datasets demonstrate substantial improvements over state-of-the-art methods in terms of robustness and efficiency.},
	urldate = {2023-11-25},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	author = {Song, Hwanjun and Kim, Minseok and Park, Dongmin and Shin, Yooju and Lee, Jae-Gil},
	month = aug,
	year = {2021},
	note = {arXiv:2012.04337 [cs]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {1490--1500},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/9TAPKLN3/Song et al. - 2021 - Robust Learning by Self-Transition for Handling No.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/MTXD3ANS/2012.html:text/html},
}

@misc{PapersCodeCIFAR100N,
	title = {Papers with {Code} - {CIFAR}-{100N} {Benchmark} ({Learning} with noisy labels)},
	url = {https://paperswithcode.com/sota/learning-with-noisy-labels-on-cifar-100n},
	abstract = {The current state-of-the-art on CIFAR-100N is PGDF. See a full comparison of 23 papers with code.},
	language = {en},
	urldate = {2023-11-25},
	keywords = {🔴},
	file = {Snapshot:/home/jeremy/Zotero/storage/VH58792T/learning-with-noisy-labels-on-cifar-100n.html:text/html},
}

@misc{CIFAR10CIFAR100Datasets,
	title = {{CIFAR}-10 and {CIFAR}-100 datasets},
	url = {https://www.cs.toronto.edu/~kriz/cifar.html},
	urldate = {2023-12-15},
	keywords = {🔴},
	file = {CIFAR-10 and CIFAR-100 datasets:/home/jeremy/Zotero/storage/F2UYRJ5Z/cifar.html:text/html},
}

@misc{bahriDeepKNNNoisy2020,
	title = {Deep k-{NN} for {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2004.12289},
	abstract = {Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple k-nearest neighbor-based ﬁltering approach on the logit layer of a preliminary model can remove mislabeled training data and produce more accurate models than many recently proposed methods. We also provide new statistical guarantees into its efﬁcacy.},
	language = {en},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Bahri, Dara and Jiang, Heinrich and Gupta, Maya},
	month = apr,
	year = {2020},
	note = {arXiv:2004.12289 [cs, stat]},
	keywords = {🔴, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Bahri et al. - 2020 - Deep k-NN for Noisy Labels.pdf:/home/jeremy/Zotero/storage/T6M4R3GF/Bahri et al. - 2020 - Deep k-NN for Noisy Labels.pdf:application/pdf},
}

@article{xiaAdaptiveGeneralModel2022,
	title = {An adaptive and general model for label noise detection using relative probabilistic density},
	volume = {239},
	issn = {0950-7051},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121010637},
	doi = {10.1016/j.knosys.2021.107907},
	abstract = {We present a model, called relative probability density (RPD), to detect label noise by utilizing the contrasting characteristics in different classes. RPD has a natural ratio structure so that a powerful measurement, the Kullback–Leibler Importance Estimation Procedure (KLIEP), can be directly applied for its calculation instead of calculating the probability density in the numerator and denominator separately. In addition, the RPD model can be reduced to a new form that contains only P(Y{\textbar}X) and can be calculated with only a probabilistic classifier and without relying on any other specific measurements, specific loss functions, noise estimation or other extra parameters. Furthermore, an RPD-based filter learning framework, which can adaptively optimize the threshold to accurately identify label noise, is proposed. The experimental results on synthetic and real data sets demonstrate that the RPD-based filter learning framework is more effective than some representative methods. The superior generality and adaptiveness, in addition to the simple design, make it a good replacement for traditional probabilistic classifiers on label-noisy data.},
	urldate = {2023-12-17},
	journal = {Knowledge-Based Systems},
	author = {Xia, Shuyin and Huang, Longhai and Wang, Guoyin and Gao, Xinbo and Shao, Yabin and Chen, Zizhong},
	month = mar,
	year = {2022},
	keywords = {🔴, Classification, Label noise, Relative probabilistic density},
	pages = {107907},
	file = {ScienceDirect Snapshot:/home/jeremy/Zotero/storage/3JW2HAQM/S0950705121010637.html:text/html},
}

@misc{englessonGeneralizedJensenShannonDivergence2021,
	title = {Generalized {Jensen}-{Shannon} {Divergence} {Loss} for {Learning} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2105.04522},
	doi = {10.48550/arXiv.2105.04522},
	abstract = {Prior works have found it beneficial to combine provably noise-robust loss functions e.g., mean absolute error (MAE) with standard categorical loss function e.g. cross entropy (CE) to improve their learnability. Here, we propose to use Jensen-Shannon divergence as a noise-robust loss function and show that it interestingly interpolate between CE and MAE with a controllable mixing parameter. Furthermore, we make a crucial observation that CE exhibit lower consistency around noisy data points. Based on this observation, we adopt a generalized version of the Jensen-Shannon divergence for multiple distributions to encourage consistency around data points. Using this loss function, we show state-of-the-art results on both synthetic (CIFAR), and real-world (e.g., WebVision) noise with varying noise rates.},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Englesson, Erik and Azizpour, Hossein},
	month = oct,
	year = {2021},
	note = {arXiv:2105.04522 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/2FDTDGZQ/Englesson and Azizpour - 2021 - Generalized Jensen-Shannon Divergence Loss for Lea.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/MBYH99EM/2105.html:text/html},
}

@misc{xuL_DMIInformationtheoreticNoiserobust2019,
	title = {L\_DMI: {An} {Information}-theoretic {Noise}-robust {Loss} {Function}},
	shorttitle = {L\_DMI},
	url = {http://arxiv.org/abs/1909.03388},
	doi = {10.48550/arXiv.1909.03388},
	abstract = {Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g. , knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, \${\textbackslash}mathcal\{L\}\_\{DMI\}\$, for training deep neural networks robust to label noise. The core of \${\textbackslash}mathcal\{L\}\_\{DMI\}\$ is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. {\textbackslash}emph\{To the best of our knowledge, \${\textbackslash}mathcal\{L\}\_\{DMI\}\$ is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information\}. In addition to theoretical justification, we also empirically show that using \${\textbackslash}mathcal\{L\}\_\{DMI\}\$ outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M. Codes are available at https://github.com/Newbeeer/L\_DMI .},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Xu, Yilun and Cao, Peng and Kong, Yuqing and Wang, Yizhou},
	month = nov,
	year = {2019},
	note = {arXiv:1909.03388 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/588JCCUN/Xu et al. - 2019 - L_DMI An Information-theoretic Noise-robust Loss .pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/H35GGACK/1909.html:text/html},
}

@article{sejutiHybridCNNKNN2023a,
	title = {A hybrid {CNN}–{KNN} approach for identification of {COVID}-19 with 5-fold cross validation},
	volume = {4},
	issn = {2666-3511},
	url = {https://www.sciencedirect.com/science/article/pii/S2666351123000037},
	doi = {10.1016/j.sintl.2023.100229},
	abstract = {The novel coronavirus is the new member of the SARS family, which can cause mild to severe infection in the lungs and other vital organs like the heart, kidney and liver. For detecting COVID-19 from images, traditional ANN can be employed. This method begins by extracting the features and then feeding the features into a suitable classifier. The classification rate is not so high as feature extraction is dependent on the experimenters' expertise. To solve this drawback, a hybrid CNN–KNN-based model with 5-fold cross-validation is proposed to classify covid-19 or non-covid19 from CT scans of patients. At first, some pre-processing steps like contrast enhancement, median filtering, data augmentation, and image resizing are performed. Secondly, the entire dataset is divided into five equal sections or folds for training and testing. By doing 5-fold cross-validation, the generalization of the dataset is ensured and the overfitting of the network is prevented. The proposed CNN model consists of four convolutional layers, four max-pooling layers, and two fully connected layers combined with 23 layers. The CNN architecture is used as a feature extractor in this case. The features are taken from the CNN model's fourth convolutional layer and finally, the features are classified using K Nearest Neighbor rather than softmax for better accuracy. The proposed method is conducted over an augmented dataset of 4085 CT scan images. The average accuracy, precision, recall and F1 score of the proposed method after performing a 5-fold cross-validation is 98.26\%, 99.42\%,97.2\% and 98.19\%, respectively. The proposed method's accuracy is comparable with the existing works described further, where the state of the art and the custom CNN models were used. Hence, this proposed method can diagnose the COVID-19 patients with higher efficiency.},
	urldate = {2023-12-17},
	journal = {Sensors International},
	author = {Sejuti, Zarin Anjuman and Islam, Md Saiful},
	month = jan,
	year = {2023},
	keywords = {🔴, Convolutional neural network, Coronavirus, Data augmentation, K fold cross-validation, K-nearest neighbor},
	pages = {100229},
	file = {Full Text:/home/jeremy/Zotero/storage/IT2M852C/Sejuti and Islam - 2023 - A hybrid CNN–KNN approach for identification of CO.pdf:application/pdf;ScienceDirect Snapshot:/home/jeremy/Zotero/storage/WF9RV4JK/S2666351123000037.html:text/html},
}

@misc{liuEarlyLearningRegularizationPrevents2020,
	title = {Early-{Learning} {Regularization} {Prevents} {Memorization} of {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2007.00151},
	doi = {10.48550/arXiv.2007.00151},
	abstract = {We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an "early learning" phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art.},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
	month = oct,
	year = {2020},
	note = {arXiv:2007.00151 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/8AY2IC5Y/Liu et al. - 2020 - Early-Learning Regularization Prevents Memorizatio.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/M4PIBFNK/2007.html:text/html},
}

@misc{baiUnderstandingImprovingEarly2021,
	title = {Understanding and {Improving} {Early} {Stopping} for {Learning} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/2106.15853},
	abstract = {The memorization e ect of deep neural network (DNN) plays a pivotal role in many stateof-the-art label-noise learning methods. To exploit this property, the early stopping trick, which stops the optimization at the early stage of training, is usually adopted. Current methods generally decide the early stopping point by considering a DNN as a whole. However, a DNN can be considered as a composition of a series of layers, and we nd that the latter layers in a DNN are much more sensitive to label noise, while their former counterparts are quite robust. Therefore, selecting a stopping point for the whole network may make di erent DNN layers antagonistically a ected each other, thus degrading the nal performance. In this paper, we propose to separate a DNN into di erent parts and progressively train them to address this problem. Instead of the early stopping, which trains a whole DNN all at once, we initially train former DNN layers by optimizing the DNN with a relatively large number of epochs. During training, we progressively train the latter DNN layers by using a smaller number of epochs with the preceding layers xed to counteract the impact of noisy labels. We term the proposed method as progressive early stopping (PES). Despite its simplicity, compared with the early stopping, PES can help to obtain more promising and stable results. Furthermore, by combining PES with existing approaches on noisy label training, we achieve state-of-the-art performance on image classi cation benchmarks.},
	language = {en},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Bai, Yingbin and Yang, Erkun and Han, Bo and Yang, Yanhua and Li, Jiatong and Mao, Yinian and Niu, Gang and Liu, Tongliang},
	month = dec,
	year = {2021},
	note = {arXiv:2106.15853 [cs]},
	keywords = {🔴, Computer Science - Machine Learning},
	file = {Bai et al. - 2021 - Understanding and Improving Early Stopping for Lea.pdf:/home/jeremy/Zotero/storage/GD6VUSZY/Bai et al. - 2021 - Understanding and Improving Early Stopping for Lea.pdf:application/pdf},
}

@misc{GJSLossesPy,
	title = {{GJS}/losses.py at main · {ErikEnglesson}/{GJS}},
	url = {https://github.com/ErikEnglesson/GJS/blob/main/losses.py},
	urldate = {2023-12-17},
	keywords = {🔴},
	file = {GJS/losses.py at main · ErikEnglesson/GJS:/home/jeremy/Zotero/storage/PIVA2CQD/losses.html:text/html},
}

@misc{wangSymmetricCrossEntropy2019,
	title = {Symmetric {Cross} {Entropy} for {Robust} {Learning} with {Noisy} {Labels}},
	url = {http://arxiv.org/abs/1908.06112},
	abstract = {Training accurate deep neural networks (DNNs) in the presence of noisy labels is an important and challenging task. Though a number of approaches have been proposed for learning with noisy labels, many open issues remain. In this paper, we show that DNN learning with Cross Entropy (CE) exhibits overﬁtting to noisy labels on some classes (“easy” classes), but more surprisingly, it also suffers from signiﬁcant under learning on some other classes (“hard” classes). Intuitively, CE requires an extra term to facilitate learning of hard classes, and more importantly, this term should be noise tolerant, so as to avoid overﬁtting to noisy labels. Inspired by the symmetric KL-divergence, we propose the approach of Symmetric cross entropy Learning (SL), boosting CE symmetrically with a noise robust counterpart Reverse Cross Entropy (RCE). Our proposed SL approach simultaneously addresses both the under learning and overﬁtting problem of CE in the presence of noisy labels. We provide a theoretical analysis of SL and also empirically show, on a range of benchmark and real-world datasets, that SL outperforms state-of-the-art methods. We also show that SL can be easily incorporated into existing methods in order to further enhance their performance.},
	language = {en},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Wang, Yisen and Ma, Xingjun and Chen, Zaiyi and Luo, Yuan and Yi, Jinfeng and Bailey, James},
	month = aug,
	year = {2019},
	note = {arXiv:1908.06112 [cs, stat]},
	keywords = {🔴, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Wang et al. - 2019 - Symmetric Cross Entropy for Robust Learning with N.pdf:/home/jeremy/Zotero/storage/SJSU78I7/Wang et al. - 2019 - Symmetric Cross Entropy for Robust Learning with N.pdf:application/pdf},
}

@misc{leeCleanNetTransferLearning2018,
	title = {{CleanNet}: {Transfer} {Learning} for {Scalable} {Image} {Classifier} {Training} with {Label} {Noise}},
	shorttitle = {{CleanNet}},
	url = {http://arxiv.org/abs/1711.07131},
	doi = {10.48550/arXiv.1711.07131},
	abstract = {In this paper, we study the problem of learning image classification models with label noise. Existing approaches depending on human supervision are generally not scalable as manually identifying correct or incorrect labels is time-consuming, whereas approaches not relying on human supervision are scalable but less effective. To reduce the amount of human supervision for label noise cleaning, we introduce CleanNet, a joint neural embedding network, which only requires a fraction of the classes being manually verified to provide the knowledge of label noise that can be transferred to other classes. We further integrate CleanNet and conventional convolutional neural network classifier into one framework for image classification learning. We demonstrate the effectiveness of the proposed algorithm on both of the label noise detection task and the image classification on noisy data task on several large-scale datasets. Experimental results show that CleanNet can reduce label noise detection error rate on held-out classes where no human supervision available by 41.5\% compared to current weakly supervised methods. It also achieves 47\% of the performance gain of verifying all images with only 3.2\% images verified on an image classification task. Source code and dataset will be available at kuanghuei.github.io/CleanNetProject.},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Lee, Kuang-Huei and He, Xiaodong and Zhang, Lei and Yang, Linjun},
	month = mar,
	year = {2018},
	note = {arXiv:1711.07131 [cs]},
	keywords = {🔴, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeremy/Zotero/storage/M3F9BN3U/Lee et al. - 2018 - CleanNet Transfer Learning for Scalable Image Cla.pdf:application/pdf;arXiv.org Snapshot:/home/jeremy/Zotero/storage/SQGYX3G7/1711.html:text/html},
}
